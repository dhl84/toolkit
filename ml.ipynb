{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import xgboost as xgb\n",
    "from fuzzywuzzy import fuzz\n",
    "import logging\n",
    "import re\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def is_valid_url(url):\n",
    "    return isinstance(url, str) and url.startswith(('http://', 'https://'))\n",
    "\n",
    "def fuzzy_url_match(url1, url2, threshold=80):\n",
    "    return fuzz.ratio(url1, url2) >= threshold\n",
    "\n",
    "def get_filename_without_extension(url):\n",
    "    return os.path.splitext(os.path.basename(url))[0]\n",
    "\n",
    "def is_filename_consistent(preview_url, full_url):\n",
    "    preview_filename = get_filename_without_extension(preview_url).replace('_PREVIEW', '')\n",
    "    full_filename = get_filename_without_extension(full_url)\n",
    "    return preview_filename == full_filename\n",
    "\n",
    "def mp4_url_exists(url):\n",
    "    if pd.isna(url):\n",
    "        logger.warning(f\"Invalid URL (NaN value)\")\n",
    "        return False\n",
    "    try:\n",
    "        response = requests.head(url, timeout=5, allow_redirects=True)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            content_type = response.headers.get('Content-Type', '').lower()\n",
    "            \n",
    "            valid_types = ['video/mp4', 'application/octet-stream', 'binary/octet-stream']\n",
    "            if any(t in content_type for t in valid_types):\n",
    "                return True\n",
    "            else:\n",
    "                logger.warning(f\"URL exists but content type is {content_type}: {url}\")\n",
    "                return False\n",
    "        elif response.status_code == 404:\n",
    "            logger.info(f\"File not found (404): {url}\")\n",
    "            return False\n",
    "        else:\n",
    "            logger.warning(f\"URL returned status code {response.status_code}: {url}\")\n",
    "            return False\n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f\"Error checking URL: {e}: {url}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error checking URL: {e}: {url}\")\n",
    "        return False\n",
    "\n",
    "def extract_url_features(url):\n",
    "    if pd.isna(url):\n",
    "        return {\n",
    "            'scheme': '',\n",
    "            'netloc': '',\n",
    "            'path': '',\n",
    "            'params': '',\n",
    "            'query': '',\n",
    "            'fragment': '',\n",
    "            'path_length': 0,\n",
    "            'num_directories': 0,\n",
    "            'file_extension': '',\n",
    "        }\n",
    "    parsed = urlparse(url)\n",
    "    return {\n",
    "        'scheme': parsed.scheme,\n",
    "        'netloc': parsed.netloc,\n",
    "        'path': parsed.path,\n",
    "        'params': parsed.params,\n",
    "        'query': parsed.query,\n",
    "        'fragment': parsed.fragment,\n",
    "        'path_length': len(parsed.path),\n",
    "        'num_directories': len([x for x in parsed.path.split('/') if x]),\n",
    "        'file_extension': os.path.splitext(parsed.path)[-1],\n",
    "    }\n",
    "\n",
    "def get_filename_without_preview(url):\n",
    "    if pd.isna(url):\n",
    "        return ''\n",
    "    filename = os.path.basename(url)\n",
    "    return re.sub(r'(_PREVIEW|_preview|sample_|_sample)', '', filename)\n",
    "\n",
    "def simple_inference(url):\n",
    "    if pd.isna(url):\n",
    "        return ''\n",
    "    parts = url.split('/')\n",
    "    filename = get_filename_without_preview(parts[-1])\n",
    "    return '/'.join(parts[:-1] + [filename])\n",
    "\n",
    "class URLRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_neighbors=5):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.knn = KNeighborsRegressor(n_neighbors=n_neighbors, metric='cosine')\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        self.n_neighbors = min(self.n_neighbors, len(y))  # Ensure n_neighbors doesn't exceed sample size\n",
    "        self.knn = KNeighborsRegressor(n_neighbors=self.n_neighbors, metric='cosine')\n",
    "        self.knn.fit(X, range(len(y)))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        try:\n",
    "            indices = self.knn.predict(X)\n",
    "            return [self.y_train[min(int(i), len(self.y_train) - 1)] for i in indices]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in URLRegressor predict: {e}\")\n",
    "            return ['' for _ in range(len(X))]  # Return empty strings if prediction fails\n",
    "\n",
    "def prepare_data(df):\n",
    "    # Extract URL features\n",
    "    url_features = df['Video_URL'].apply(extract_url_features)\n",
    "    \n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    url_features_df = pd.DataFrame(url_features.tolist(), index=df.index)\n",
    "    \n",
    "    # Concatenate the original DataFrame with the new features\n",
    "    X = pd.concat([df[['Video_URL']], url_features_df], axis=1)\n",
    "    y = df['New_URL']\n",
    "    \n",
    "    # Handle NaN values\n",
    "    X = X.fillna('')\n",
    "    y = y.fillna('')\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "class KNNURLRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_neighbors=5):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.knn = KNeighborsRegressor(n_neighbors=n_neighbors, metric='cosine')\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X.toarray() if hasattr(X, 'toarray') else X\n",
    "        self.y_train = y\n",
    "        self.y_encoded = self.label_encoder.fit_transform(y)\n",
    "        self.n_neighbors = min(self.n_neighbors, len(y))\n",
    "        self.knn.fit(self.X_train, self.y_encoded)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        try:\n",
    "            X = X.toarray() if hasattr(X, 'toarray') else X\n",
    "            distances, indices = self.knn.kneighbors(X)\n",
    "            predictions = []\n",
    "            for i, row in enumerate(X):\n",
    "                for idx in indices[i]:\n",
    "                    predicted_url = self.y_train[idx]\n",
    "                    if is_filename_consistent(self.X_train[i]['Video_URL'], predicted_url):\n",
    "                        predictions.append(predicted_url)\n",
    "                        break\n",
    "                else:\n",
    "                    predictions.append('')  # No consistent match found\n",
    "            return predictions\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in KNNURLRegressor predict: {e}\")\n",
    "            return ['' for _ in range(len(X))]\n",
    "\n",
    "class XGBoostURLRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.model = xgb.XGBClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            objective='multi:softprob'\n",
    "        )\n",
    "        \n",
    "        # Define preprocessing steps\n",
    "        categorical_features = ['scheme', 'netloc', 'file_extension']\n",
    "        numeric_features = ['path_length', 'num_directories']\n",
    "        \n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "        \n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features)\n",
    "            ])\n",
    "        \n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        logger.info(f\"Fitting XGBoostURLRegressor with {len(X)} samples\")\n",
    "        logger.info(f\"X columns: {X.columns}\")\n",
    "        logger.info(f\"X dtypes: {X.dtypes}\")\n",
    "        \n",
    "        # Ensure X doesn't contain the 'Video_URL' column\n",
    "        if 'Video_URL' in X.columns:\n",
    "            X = X.drop('Video_URL', axis=1)\n",
    "        \n",
    "        # Preprocess the data\n",
    "        X_preprocessed = self.preprocessor.fit_transform(X)\n",
    "        \n",
    "        # Encode target variable\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        \n",
    "        logger.info(f\"Preprocessed X shape: {X_preprocessed.shape}\")\n",
    "        logger.info(f\"y_encoded shape: {y_encoded.shape}\")\n",
    "        \n",
    "        # Fit the model\n",
    "        self.model.fit(X_preprocessed, y_encoded)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        logger.info(f\"Predicting with XGBoostURLRegressor for {len(X)} samples\")\n",
    "        logger.info(f\"X columns: {X.columns}\")\n",
    "        logger.info(f\"X dtypes: {X.dtypes}\")\n",
    "        \n",
    "        try:\n",
    "            # Ensure X doesn't contain the 'Video_URL' column\n",
    "            if 'Video_URL' in X.columns:\n",
    "                X = X.drop('Video_URL', axis=1)\n",
    "            \n",
    "            # Preprocess the data\n",
    "            X_preprocessed = self.preprocessor.transform(X)\n",
    "            \n",
    "            logger.info(f\"Preprocessed X shape for prediction: {X_preprocessed.shape}\")\n",
    "            \n",
    "            # Make predictions\n",
    "            y_encoded_pred = self.model.predict(X_preprocessed)\n",
    "            predictions = self.label_encoder.inverse_transform(y_encoded_pred)\n",
    "            \n",
    "            logger.info(f\"Made {len(predictions)} predictions\")\n",
    "            \n",
    "            return predictions\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in XGBoostURLRegressor predict: {e}\")\n",
    "            return np.array([''] * len(X))\n",
    "\n",
    "class ImprovedURLRegressor:\n",
    "    def __init__(self, n_neighbors=5):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.knn = KNeighborsRegressor(n_neighbors=n_neighbors, metric='cosine')\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        logger.info(f\"Fitting ImprovedURLRegressor. Input shapes - X: {X.shape}, y: {y.shape}\")\n",
    "        valid_mask = X['Video_URL'].apply(is_valid_url)\n",
    "        self.X_train = X[valid_mask]\n",
    "        self.y_train = y[valid_mask]\n",
    "        logger.info(f\"After filtering invalid URLs - X_train: {self.X_train.shape}, y_train: {self.y_train.shape}\")\n",
    "        \n",
    "        self.n_neighbors = min(self.n_neighbors, len(self.y_train))\n",
    "        self.knn.fit(self.X_train.drop('Video_URL', axis=1), range(len(self.y_train)))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        logger.info(f\"Predicting with ImprovedURLRegressor. Input shape - X: {X.shape}\")\n",
    "        try:\n",
    "            valid_mask = X['Video_URL'].apply(is_valid_url)\n",
    "            X_valid = X[valid_mask]\n",
    "            logger.info(f\"Valid prediction data shape: {X_valid.shape}\")\n",
    "            \n",
    "            distances, indices = self.knn.kneighbors(X_valid.drop('Video_URL', axis=1))\n",
    "            logger.info(f\"KNN prediction indices shape: {indices.shape}\")\n",
    "            \n",
    "            predictions = []\n",
    "            for i, row in X_valid.iterrows():\n",
    "                for idx in indices[i]:\n",
    "                    predicted_url = self.y_train.iloc[idx]\n",
    "                    if is_filename_consistent(row['Video_URL'], predicted_url) and fuzzy_url_match(row['Video_URL'], predicted_url):\n",
    "                        predictions.append(predicted_url)\n",
    "                        break\n",
    "                else:\n",
    "                    predictions.append('')\n",
    "            \n",
    "            logger.info(f\"Number of predictions made: {len(predictions)}\")\n",
    "            \n",
    "            result = pd.Series([''] * len(X), index=X.index)\n",
    "            result[X_valid.index] = predictions\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in ImprovedURLRegressor predict: {e}\")\n",
    "            return pd.Series([''] * len(X), index=X.index)\n",
    "\n",
    "def train_and_predict_iteratively(csv_path, start_iteration=0):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        logger.info(f\"Loaded {len(df)} rows from CSV\")\n",
    "        \n",
    "        # Initialize sets for known and unknown pairs\n",
    "        known_pairs = df[df['Works'] == True].copy()\n",
    "        unknown_pairs = df[df['Works'].isin([False, np.nan])].copy()\n",
    "        \n",
    "        logger.info(f\"Initial known pairs: {len(known_pairs)}, unknown pairs: {len(unknown_pairs)}\")\n",
    "        \n",
    "        # Iterative prediction process\n",
    "        for iteration, method in enumerate(['simple', 'knn', 'xgboost'][start_iteration:], start=start_iteration):\n",
    "            logger.info(f\"Starting iteration {iteration + 1} with method: {method}\")\n",
    "            \n",
    "            if len(unknown_pairs) == 0:\n",
    "                logger.info(\"All pairs resolved. Stopping iterations.\")\n",
    "                break\n",
    "            \n",
    "            # Prepare data\n",
    "            X_known, y_known = prepare_data(known_pairs)\n",
    "            X_unknown, _ = prepare_data(unknown_pairs)\n",
    "            \n",
    "            logger.info(f\"Prepared data - X_known: {X_known.shape}, X_unknown: {X_unknown.shape}\")\n",
    "            logger.info(f\"X_known dtypes: {X_known.dtypes}\")\n",
    "            logger.info(f\"y_known dtype: {y_known.dtype}\")\n",
    "            \n",
    "            # Predict using the current method\n",
    "            if method == 'simple':\n",
    "                null_works = unknown_pairs['Works'].isnull()\n",
    "                predictions = unknown_pairs.loc[null_works, 'Video_URL'].apply(simple_inference)\n",
    "                unknown_pairs.loc[null_works, 'Predicted_New_URL'] = predictions\n",
    "            elif method == 'knn':\n",
    "                model = KNNURLRegressor(n_neighbors=5)\n",
    "                model.fit(X_known, y_known)\n",
    "                predictions = model.predict(X_unknown)\n",
    "                unknown_pairs['Predicted_New_URL'] = predictions\n",
    "            elif method == 'xgboost':\n",
    "                model = XGBoostURLRegressor()\n",
    "                model.fit(X_known.drop('Video_URL', axis=1), y_known)\n",
    "                predictions = model.predict(X_unknown.drop('Video_URL', axis=1))\n",
    "                unknown_pairs['Predicted_New_URL'] = predictions\n",
    "            \n",
    "            logger.info(f\"Made {len(predictions)} predictions\")\n",
    "            \n",
    "            # Validate predictions\n",
    "            unknown_pairs['Prediction_Works'] = unknown_pairs.apply(lambda row: \n",
    "                mp4_url_exists(row['Predicted_New_URL']) and \n",
    "                is_filename_consistent(row['Video_URL'], row['Predicted_New_URL']) and\n",
    "                fuzzy_url_match(row['Video_URL'], row['Predicted_New_URL']), axis=1)\n",
    "            \n",
    "            # Move successful predictions to known pairs\n",
    "            successful_predictions = unknown_pairs[unknown_pairs['Prediction_Works'] == True]\n",
    "            known_pairs = pd.concat([known_pairs, successful_predictions])\n",
    "            unknown_pairs = unknown_pairs[unknown_pairs['Prediction_Works'] == False]\n",
    "            \n",
    "            logger.info(f\"Iteration {iteration + 1} complete. \"\n",
    "                        f\"Successful predictions: {len(successful_predictions)}, \"\n",
    "                        f\"Remaining unknown pairs: {len(unknown_pairs)}\")\n",
    "            \n",
    "            # Write results after each iteration\n",
    "            all_pairs = pd.concat([known_pairs, unknown_pairs])\n",
    "            all_pairs['Final_URL'] = all_pairs.apply(lambda row: row['New_URL'] if row['Works'] == True else row['Predicted_New_URL'], axis=1)\n",
    "            all_pairs.to_csv(f'iterative_url_predictions_iter_{iteration+1}.csv', index=False)\n",
    "            logger.info(f\"Results saved to 'iterative_url_predictions_iter_{iteration+1}.csv'\")\n",
    "        \n",
    "        return all_pairs\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in train_and_predict_iteratively: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Usage\n",
    "# csv_path = 'db.csv'\n",
    "# results = train_and_predict_iteratively(csv_path)\n",
    "# csv_path = 'iterative_url_predictions_iter_1.csv'\n",
    "csv_path = 'iterative_url_predictions_iter_3.csv'\n",
    "results = train_and_predict_iteratively(csv_path, start_iteration=2)  # Start from XGBoost\n",
    "\n",
    "if results is not None:\n",
    "    print(f\"Total pairs: {len(results)}\")\n",
    "    print(f\"Successfully predicted pairs: {results['Prediction_Works'].sum()}\")\n",
    "    print(f\"Remaining unknown pairs: {len(results[results['Works'].isin([False, np.nan])])}\")\n",
    "else:\n",
    "    print(\"Prediction process failed. Please check the logs for more information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def analyze_xgboost_predictions(csv_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    logger.info(f\"Loaded {len(df)} rows from {csv_path}\")\n",
    "\n",
    "    # Filter for successful predictions\n",
    "    successful_predictions = df[df['Prediction_Works'] == True]\n",
    "    logger.info(f\"Found {len(successful_predictions)} successful predictions\")\n",
    "\n",
    "    if len(successful_predictions) > 0:\n",
    "        logger.info(\"Details of successful predictions:\")\n",
    "        for index, row in successful_predictions.iterrows():\n",
    "            logger.info(f\"Index: {index}\")\n",
    "            logger.info(f\"Video URL: {row['Video_URL']}\")\n",
    "            logger.info(f\"Predicted New URL: {row['Predicted_New_URL']}\")\n",
    "            logger.info(f\"Original New URL: {row['New_URL']}\")\n",
    "            logger.info(\"---\")\n",
    "    else:\n",
    "        logger.info(\"No successful predictions found.\")\n",
    "\n",
    "    # Additional analysis\n",
    "    logger.info(f\"Total pairs: {len(df)}\")\n",
    "    logger.info(f\"Known pairs (Works == True): {len(df[df['Works'] == True])}\")\n",
    "    logger.info(f\"Unknown pairs (Works == False or NaN): {len(df[df['Works'].isin([False, pd.NA])])}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = 'iterative_url_predictions_iter_3.csv'\n",
    "    analyze_xgboost_predictions(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def consolidate_results(original_csv, ml_inferred_csv, output_csv):\n",
    "    # Load the original and ML-inferred datasets\n",
    "    original_df = pd.read_csv(original_csv)\n",
    "    ml_inferred_df = pd.read_csv(ml_inferred_csv)\n",
    "    \n",
    "    # Merge the datasets based on the Video_URL\n",
    "    merged_df = pd.merge(original_df, ml_inferred_df[['Video_URL', 'ML_Predicted_New_URL', 'ML_Prediction_Works']], \n",
    "                         on='Video_URL', how='left')\n",
    "    \n",
    "    # Update the New_URL and Works columns where ML prediction worked\n",
    "    mask = (merged_df['Works'] == False) & (merged_df['ML_Prediction_Works'] == True)\n",
    "    merged_df.loc[mask, 'New_URL'] = merged_df.loc[mask, 'ML_Predicted_New_URL']\n",
    "    merged_df.loc[mask, 'Works'] = True\n",
    "    \n",
    "    # Drop the ML-specific columns\n",
    "    merged_df = merged_df.drop(columns=['ML_Predicted_New_URL', 'ML_Prediction_Works'])\n",
    "    \n",
    "    # Save the consolidated dataset\n",
    "    merged_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Consolidated results saved to {output_csv}\")\n",
    "    \n",
    "    # Print statistics\n",
    "    total_updated = mask.sum()\n",
    "    print(f\"Total records updated: {total_updated}\")\n",
    "    print(f\"Percentage of previously non-working URLs now working: {total_updated / (original_df['Works'] == False).sum():.2%}\")\n",
    "\n",
    "# Usage\n",
    "consolidate_results('db.csv', 'ml_inferred_urls.csv', 'consolidated_db.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
